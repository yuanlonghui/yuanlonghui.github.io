{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc0e6db",
   "metadata": {},
   "source": [
    "# Indexing in torch\n",
    "\n",
    "## common indexing\n",
    "\n",
    "利用整数坐标 $index$，或者切片 $start:end:step$ 进行索引。两者可组合使用。\n",
    "\n",
    "使用整数坐标时，该维度将会消失（取出元素动作）。\n",
    "\n",
    "使用切片时，该维度会被保留（切取片段）。\n",
    "\n",
    "## bool indexing\n",
    "\n",
    "通过 *bool tensor* 作为索引，索引至位置为 True 的位置，可进行赋值和取值。\n",
    "\n",
    "在 bool indexing 和前述 common indexing 混用时，优先处理完 common indexing，剩余维度继续匹配 bool indexing。\n",
    "\n",
    "当 *bool tensor* 为 1-dim 时，其大小须等于被索引维度的大小，下标对应 True 的位置被索引到。\n",
    "\n",
    "当 *bool tensor* 为 n-dim 时，匹配剩余维度之后（严格每一维大小相等），将其余维度视作整体元素，剩余维度之中下标对应 True 的位置被索引到。(建议不要在 *bool indexing* 之后的维度用 *common indexing*，要使用也请选择先 transpose 交换维度顺序，不然代码行为会非常怪异）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fae3aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5820, -2.7869],\n",
      "         [ 0.4172, -0.2438]],\n",
      "\n",
      "        [[-0.8403,  0.3157],\n",
      "         [-0.3307, -0.4388]],\n",
      "\n",
      "        [[-0.3768, -1.1343],\n",
      "         [-1.3863,  0.1587]]])\n",
      "tensor([[[-0.3768, -1.1343],\n",
      "         [-1.3863,  0.1587]]])\n",
      "tensor([[-2.7869],\n",
      "        [ 0.3157],\n",
      "        [-1.1343]])\n",
      "tensor([[ 0.4172, -0.2438],\n",
      "        [-0.8403,  0.3157],\n",
      "        [-0.3768, -1.1343],\n",
      "        [-1.3863,  0.1587]])\n",
      "tensor([-0.2438, -0.3307, -1.3863,  0.1587])\n",
      "tensor([-0.2438,  0.3157, -1.1343,  0.1587])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn(3, 2, 2)\n",
    "\n",
    "index_0 = torch.tensor([False, False, True])\n",
    "index_1 = torch.tensor([True, False])\n",
    "index_2 = torch.tensor([[False, True], [True, False], [True, True]])\n",
    "print(a)\n",
    "# 比较有用的用法\n",
    "print(a[index_0])\n",
    "print(a[:, index_1, 1])\n",
    "print(a[index_2])\n",
    "\n",
    "# 先在 dim=1 上 索引位置 1，然后剩下 dim=[0,2] 匹配 index_2\n",
    "print(a[index_2, 1])\n",
    "\n",
    "# 没有找到一种一次索引写法可以先索引 dim=2 处位置 1，然后再将剩下的 dim=[0,1] 匹配 index2\n",
    "# 不过可以写成以下方式\n",
    "print(a[index_2][:, 1])  # 或者 print(a[:, :, 1][index_2])\n",
    "# 没意义，这代码非常令人迷惑。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70db02f",
   "metadata": {},
   "source": [
    "## fancy indexing\n",
    "\n",
    "花式索引，使用一组 *long tensor* 进行索引。\n",
    "\n",
    "条件：组内的 *long tensor* 能够广播至同一形状，每个元素所在的 *long tensor* 想要索引的维度不越界。\n",
    "\n",
    "只考虑 *common indexing* 和 *fancy indexing* 的情况，其作用机制是先处理 *common indexing*，将保留维度看作整体元素，接着处理 *fancy indexing* 的维度，首先将组内向量广播至同一形状，然后将其每个对应位置的值组成一组坐标，按照最终广播形状索引 n 组坐标对应的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01667d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 2],\n",
      "        [1, 4]])\n",
      "tensor([[2, 1],\n",
      "        [4, 3]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, 1 ,2, 3, 4])\n",
    "idx_0 = torch.tensor([[3, 2],[1, 4]])\n",
    "print(a[idx_0])\n",
    "b = torch.tensor([[0, 1], [2, 3], [4, 5]])\n",
    "idx_0 = torch.tensor([[1, 0],[2, 1]])\n",
    "idx_1 = torch.tensor([0, 1])\n",
    "print(b[idx_0, idx_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e33307a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[True, True, True, True],\n",
      "         [True, True, True, True],\n",
      "         [True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True],\n",
      "         [True, True, True, True],\n",
      "         [True, True, True, True]]])\n",
      "tensor([[[0]],\n",
      "\n",
      "        [[1]]])\n",
      "tensor([[[0],\n",
      "         [1],\n",
      "         [2]]])\n",
      "tensor([[[0, 1, 2, 3]]])\n",
      "tensor([[[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1, 1],\n",
      "         [1, 1, 1, 1],\n",
      "         [1, 1, 1, 1]]])\n",
      "tensor([[[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1],\n",
      "         [2, 2, 2, 2]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1],\n",
      "         [2, 2, 2, 2]]])\n",
      "tensor([[[0, 1, 2, 3],\n",
      "         [0, 1, 2, 3],\n",
      "         [0, 1, 2, 3]],\n",
      "\n",
      "        [[0, 1, 2, 3],\n",
      "         [0, 1, 2, 3],\n",
      "         [0, 1, 2, 3]]])\n"
     ]
    }
   ],
   "source": [
    "# 有一个非常有意思的操作\n",
    "def arrange_at_dim(arr_size, at_dim, total_dim):\n",
    "    data = torch.arange(arr_size)\n",
    "    assert at_dim < total_dim, \"expect at_dim < total_dim\"\n",
    "    prefix = [1] * at_dim\n",
    "    suffix = [1] * (total_dim - 1 - at_dim)\n",
    "    return data.view(*prefix, -1, *suffix).contiguous()\n",
    "\n",
    "a = torch.arange(2 * 3 * 4).view(2, 3, 4).contiguous()\n",
    "# 利用这组特殊的tensor进行fancy indexing，被索引张量每一个位置都在原始位置被索引到。\n",
    "index_0 = arrange_at_dim(a.shape[0], 0, a.ndim)\n",
    "index_1 = arrange_at_dim(a.shape[1], 1, a.ndim)\n",
    "index_2 = arrange_at_dim(a.shape[2], 2, a.ndim)\n",
    "print(a[index_0, index_1, index_2] == a)\n",
    "# 原因是这组tensor在广播之后恰好构成了与原始张量形状相同，并且每组坐标正好索引到当前位置\n",
    "print(index_0, index_1, index_2, sep=\"\\n\")\n",
    "print(*torch.broadcast_tensors(index_0, index_1, index_2), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f679115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3],\n",
      "        [0],\n",
      "        [6]])\n",
      "tensor([[3],\n",
      "        [0],\n",
      "        [6]])\n",
      "tensor([[1],\n",
      "        [3],\n",
      "        [8]])\n",
      "tensor([[1],\n",
      "        [3],\n",
      "        [8]])\n"
     ]
    }
   ],
   "source": [
    "# 看看 torch.gather 和 torch.scatter 的作用机制\n",
    "# torch.gather(input, dim, index)\n",
    "# out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\n",
    "# out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\n",
    "# out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n",
    "\n",
    "# scatter(input, dim, index, src)\n",
    "# input[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\n",
    "# input[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\n",
    "# input[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
    "\n",
    "# 可以认为是将上面的 特殊组合 + 替换指定的 dim 为 index 的 fancy indexing\n",
    "\n",
    "def my_gather(input_t, dim, index):\n",
    "    all_idx = []\n",
    "    for i in range(index.ndim):\n",
    "        all_idx.append(arrange_at_dim(index.shape[i], i, input_t.ndim) if i != dim else index)\n",
    "        \n",
    "    return input_t[all_idx]\n",
    " \n",
    "rst = torch.gather(torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]]), dim=0, index=torch.tensor([[1], [0], [2]]))\n",
    "print(rst)\n",
    "print(my_gather(torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]]), dim=0, index=torch.tensor([[1], [0], [2]])))\n",
    "\n",
    "rst = torch.gather(torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]]), dim=1, index=torch.tensor([[1], [0], [2]]))\n",
    "print(rst)\n",
    "print(my_gather(torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]]), dim=1, index=torch.tensor([[1], [0], [2]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab506eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
